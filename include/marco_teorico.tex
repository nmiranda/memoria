\chapter{Marco Teórico}

% Conceptos involucrados
% - Nuevas tecnologías consideradas
% - Metodologías de desarrollo
% - Algoritmos existentes
% - Arquitecturas estándar
% - Descripción de las soluciones existentes
% Concentra la mayor parte de las citas
% Alquien que conoce del tema podría saltearse este capítulo e igual comprender la memoria

\section{Ciencia de datos: espectroscopía astronómica}

Durante el siglo XIX nace la astrofísica moderna. Fue entonces que, por primera vez, se logró medir distancias estelares; que revelaron lo lejanos que se encuentran estos objetos de la tierra. Surgió, también en aquel siglo, la \textit{espectroscopía física}, que permitió la identificación de elementos químicos a través de \textit{líneas espectrales}. A partir de esto nace la química moderna, con el descubrimiento de la tabla periódica de los elementos. Gracias a estos avances es que, posteriormente, llega a surgir la mecánica cuántica en el siglo XX y, junto con ello, la clasificación espectral de las estrellas.

En el año 1814, el científico Joseph von Fraunhofer (1787 - 1826), mediante el uso de prismas de alta calidad construidos por él mismo, logró difractar un rayo de luz solar y proyectarlo hacia un muro blanco. Además de los colores característicos del arcoíris, observados de esta manera desde los tiempos de Newton, vio en la proyección resultante muchas líneas oscuras. Procedió, luego, a catalogar meticulosamente la longitud de onda exacta de cada una de estas líneas, que hasta el día de hoy se conocen como líneas de Fraunhofer, y asignó letras a las más notorias. De esta forma, Fraunhofer registró el primer espectro astronómico de alta resolución. En la Figura \ref{fig:fraunhofer_sun} se puede apreciar un espectro de los catalogados por Fraunhofer.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.9\textwidth]{imagenes/fraunhofer_sun.png}
\end{center}
\vspace*{-5mm}
\caption{Espectro solar registrado por Fraunhofer \cite{tennyson2010astronomical}.}
\label{fig:fraunhofer_sun}
\end{figure}

Posteriormente, procedió a realizar el mismo experimento, pero esta vez utilizando un rayo de luz proveniente de la estrella roja cercana Betelgeuse, y observó que el patrón de líneas oscuras cambiaba considerablemente. Fraunhofer concluyó correctamente que estas se encuentran de cierta forma relacionadas con la composición del objeto observado. En efecto, algunas de las líneas observadas por Fraunhofer se deben a las especies (e.g átomos, iones, moléculas) que componen la atmósfera terrestre.

Sin embargo, el gran paso en la comprensión general de las observaciones de Fraunhofer llegó a mediados del siglo XIX de la mano del trabajo de los científicos Gustav Kirchhoff (1824 - 1887) y Robert Bunsen (1811 - 1899), quienes estudiaron el color de la luz emitida al poner distintos metales en llamas. Al hacer esto, descubrieron que, en ciertos casos, la longitud de onda de la luz emitida coincidía exactamente con las líneas observadas por Fraunhofer. Estos experimentos demostraron que las líneas de Fraunhofer son una consecuencia directa de la composición atómica del sol.

En el siglo XX se llegó a comprender de manera más profunda la razón de la existencia de estas líneas, denominadas \textit{líneas espectrales}, gracias a la revolución que significó la llegada de la mecánica cuántica. Los desarrollos en materia de espectroscopía han estado, desde entonces, estrechamente ligados a los de aquel campo de la física.

Si se observa cuidadosamente ciertos objetos, tales como los planetas Marte o Júpiter, o estrellas tales como Betelgeuse, se puede apreciar que estos objetos tienden a tener un cierto color. Basta utilizar instrumentos de bajo poder resolutivo para separar la luz que llega desde estos objetos a la tierra en colores de amplio espectro. A su vez, el observar estos colores entrega información sobre la temperatura del objeto. Por ejemplo, las estrellas azules poseen mayor temperatura que las rojas. Objetos que emiten rayos X, como la corona solar, son muy calientes, mientras que objetos fríos emitirán radiación en longitudes de onda mayores; por ejemplo, en forma de ondas de radio.

La mejor forma de obtener información astrofísica detallada de objetos del cielo es mediante observaciones de alta resolución espectral. Observaciones llevadas a cabo con equipos de tal capacidad permiten obtener, no solamente la posición central de una línea dentro del espectro, sino también su forma. Mediante este procedimiento se puede inferir propiedades del objeto, tales como su composición química, su temperatura, la abundancia de las especies que lo componen y que se encuentran emitiendo radiación, el movimiento de las especies y del objeto en sí, la presión y densidad local, el campo magnético presente, entre otros.

Esto se lleva a cabo con equipos de alto poder resolutivo y sensibilidad. Dos ejemplos de estos son, el telescopio óptico SDSS que se encuentra en el Apache Point Observatory (APO, ubicado en Nuevo México, Estados Unidos) y con el cual se lleva a cabo el \textit{Sloan Digital Sky Survey (SDSS)}; y, en mayor medida, el interferómetro radioastronómico \textit{Atacama Large Millimeter/submillimeter Array (ALMA)} ubicado en el norte de Chile.

\subsection{Atacama Large Millimeter Array (ALMA)}

El \textit{Atacama Large Millimeter Array (ALMA)} \cite{almaobservatory.org} \cite{wootten2009atacama} es un interferómetro de señales de radio ubicado en el desierto de Atacama, en el norte de Chile. Es un proyecto llevado a cabo mediante una asociación de organizaciones de Norteamérica, Europa y el Este de Asia. Comenzó sus observaciones científicas en la segunda mitad del año 2011. Es, por lejos, el mayor y más importante radiotelescopio construido hasta la fecha. Se encuentra realizando observaciones preliminares desde marzo del año 2013, y se espera que opere al cien por ciento de su capacidad desde marzo del 2017.

ALMA realiza observaciones captando radiación electromagnética proveniente del espacio en bandas milimétricas y submilimétricas en sus longitudes de onda, que corresponden a ondas de radio. Debido a que en condiciones normales la humedad del ambiente y del cielo absorbe gran parte de este tipo de radiación, es crucial para el funcionamiento de los telescopios el estar ubicados en un lugar seco; y el más idóneo en ese sentido es, sin dudas, el llano de Chajnantor en el desierto de Atacama, a más de 5000 metros de altura.

Debido al diseño de ALMA, en muchas de sus observaciones se detectará una abundancia de líneas espectrales; lo cual puede ser un resultado complementario al objetivo principal de una observación en particular, y por ende, puede no ser analizado por el o la astrónomo(a) que lo propuso.

Con el tiempo se espera ocurra una eventual acumulación de grandes cantidades de datos espectrales de ALMA. Esto abre la oportunidad de desarrollar nuevas técnicas de estudio basados en la minería de datos u otras técnicas de computación poco usadas por los astrónomos. De ahí que en el presente trabajo se busque implementar algoritmos de aprendizaje de reglas de asociación, o \textit{Association Rule Learning (ARL)}, para el estudio masivo de datos espectroscópicos

Gran parte de los datos obtenidos desde ALMA son guardados en estructuras de datos llamadas cubos de datos tipo ALMA (o \textit{ALMA Data Cubes}), como el que se observa en la Figura \ref{fig:data_cube}, que contienen información de distintos puntos de observación del cielo a distintas frecuencias. Los cubos de datos tipo ALMA, como estructura de datos, contienen valores indexados en tres coordenadas. Dos de las coordenadas son espaciales, y corresponden al equivalente a una imagen normal de dos dimensiones, en el sentido que describen puntos del cielo (o del espacio observable desde la tierra). La tercera coordenada corresponde al rango de frecuencias en el que se está detectando radiación electromagnética. Por lo tanto, si se fijan las dos coordenadas espaciales (se fija un punto en el espacio) y se extraen todos los valores en la tercera coordenada de aquel punto, se obtiene el espectro de frecuencias observado en ese punto del espacio.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.9\textwidth]{imagenes/data_cube.png}
\end{center}
\vspace*{-5mm}
\caption{Representación gráfica de un cubo de datos tipo ALMA. Dos de sus cordenadas son espaciales mientras la tercera corresponde al dominio de las frecuencias \cite{Eguchi:2013waa}.}
\label{fig:data_cube}
\end{figure}

A partir de ALMA se generan enormes cantidades de datos, los cuales necesariamente deben procesarse por parte de sistemas automatizados de extracción y análisis con el fin de facilitar a los investigadores el inferir información útil a partir de estos.

\subsection{Sloan Digital Sky Survey (SDSS)}

Dado que se espera obtener datos de ALMA para el uso de técnicas tales como el aprendizaje de reglas de asociación a partir del año 2017, se requiere una base de datos espectroscópicos pre-existente con el fin de poner a prueba el sistema desarrollado en el presente trabajo.

El \textit{Sloan Digital Sky Survey (SDSS)} \cite{york2000sloan} es un proyecto de inspección y estudio del espacio llevado a cabo mediante el uso de un telescopio óptico ubicado en el observatorio Apache Point (APO), Nuevo México, Estados Unidos. La recolección de datos comenzó en el año 2000, y las imágenes finales de los datos publicados cubren un 35\% del cielo, con observaciones fotométricas de 500 millones de objetos y espectros ópticos de 1 millón de objetos.

Los espectros del SDSS cubren desde 3600 a 10400 Angstroms ({\AA})\footnote{\url{https://www.sdss3.org/instruments/boss_spectrograph.php\#Parameters}} con una resolución de 1 {\AA}\footnote{$1\,\text{{\AA}} = 10^{-10}\,m = 10^{-1}\,nm$}. Los objetos estudiados son principalmente galaxias, incluyendo \textit{quásares} y \textit{AGN} (un 80\% del total de datos), y el resto son estrellas de distinto tipo (20\% del total) cuyos espectros se encuentran dominados por muchas líneas de absorción, como el que se muestra en la Figura \ref{fig:sdss_spec}. Los espectros de regiones de gas o de galaxias, por otra parte, poseen pocas líneas de absorción. El SDSS tiene en sus catálogos un universo de casi 50 líneas espectrales posibles previamente identificadas, presentes dentro de su rango de detección.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.9\textwidth]{imagenes/sdss_spec.png}
\end{center}
\vspace*{-5mm}
\caption{Espectro de frecuencia (flujo versus longitud de onda) de un objeto estelar del SDSS con us líneas identificadas. Se puede apreciar que el espectro es dominado por líneas de absorción. Nótese que existen líneas de absorción no identificadas cerca de $\lambda$ = 8800 {\AA} (imagen obtenida desde \textit{sdss.org}).}
\label{fig:sdss_spec}
\end{figure}

Los datos de SDSS se hacen disponibles mediante publicaciones regulares o \textit{data releases} a través de internet. La última publicación llevada a cabo fue la correspondiente al data release 10 (DR10), con fecha de julio del 2013. Los datos de todos los data releases se encuentran en un servidor \textit{Microsoft SQL Server} y pueden accederse mediante diversas interfaces o APIs presentes en el sitio web de SDSS. En particular, existe una interfaz web llamada \textit{CasJobs} que permite realizar consultas en lenguaje \textit{SQL} a un servidor que encola la petición, la ejecuta y guarda los resultados en una base de datos asignada al usuario.

Para probar los algoritmos y el sistema implementados en el presente trabajo, en particular, se utilizó el \textit{data release 7 (DR7)} como fuente de datos, ya que es el último \textit{data release} que contiene información completa sobre las líneas detectadas en los distintos espectros de frecuencia.

\section{Reglas de asociación}

El aprendizaje mediante reglas de asociación, o \textit{Association Rule learning (ARL)}, es sin lugar a dudas uno de los métodos más populares y mejor estudiados dentro de la minería de datos. Basta para ello ver que el artículo seminal de Agrawal et al. \cite{agrawal1993mining}, donde se sentaron las bases de la teoría subyacente, es uno de los más citados del área; según el catálogo y herramienta de búsqueda de publicaciones científicas \textit{Google Scholar}.

La motivación principal de ARL en su concepción fue el encontrar relaciones lógicas entre los artículos adquiridos por usuarios en puntos de venta del tipo \textit{"Si un cliente compra los artículos A y B, entonces es muy probable que también compre el artículo C"}. Sin embargo, la teoría de fondo que se desarrolló con el tiempo tiene una gran cantidad de aplicaciones en los más diversos ámbitos.

\subsection{Definición formal}

Sea $\mathcal{I} = \{i_1,\,i_2,\,i_3,\,\ldots ,\,i_n\}$ un universo de ítems posibles. Se denomina, entonces a un conjunto $X \subseteq \mathcal{I}$ como \textit{conjunto de ítems} o \textit{itemset}. Se tiene, además un conjunto de transacciones $\mathcal{T} = \{T_1,\,T_2,\,\ldots,\,T_m\}$, donde $T_i \subseteq \mathcal{I}, \; \forall i \in {[1,m]}$. Dados un conjunto de ítems $X$ y una transacción $T_i$, se dice que la trasacción $T_i$ \textit{satisface} $X$ si y solo si $X \subseteq T_i$.

Una \textit{regla de asociación} es, entonces, una relación entre dos conjuntos $X$ e $Y$, donde $X \subset \mathcal{I}$, $Y \subset \mathcal{I}$, y $X \cap Y = \emptyset$. Esta relación se denota de la forma $X \Rightarrow Y$. A $X$ se denomina el \textit{antecedente} de la regla y a $Y$ se denomina el \textit{consecuente} de la regla.

Existen una serie de medidas para cuantificar la relevancia de una regla de asociación. A continuación se definen algunas de ellas.

El \textit{soporte} de un conjunto de ítems $X$, o $\mathit{supp}(X)$, se define como $$\mathit{supp}(X) = \frac{|\mathcal{T}_X|}{|\mathcal{T}|} \quad \text{, tal que} \quad \mathcal{T}_X = \{T \in \mathcal{T} \, : \, X \subset T \}\text{,}$$ donde $|X|$, cuando $X$ es un conjunto finito cualquiera, es el número de elementos que posee $X$. Vale decir, el soporte corresponde a la fracción del total de transacciones en la que está presente el conjunto.

A su vez, el soporte de una regla de asociación $X \Rightarrow Y$, o $\mathit{supp}(X \Rightarrow Y)$, se define como $$\mathit{supp}(X \Rightarrow Y) = \mathit{supp}(X \cup Y)\text{,}$$ vale decir, corresponde a la fracción del total de transacciones en las cuales está presente tanto el antecedente como el consecuente de la regla simultáneamente\footnote{Debe tenerse en mente que la expresión $\mathit{supp}(X \cup Y)$ indica la fracción del total de transacciones en las cuales está presente \textbf{tanto} el antecedente como el consecuente de la regla \textbf{simultáneamente}, y \textbf{no} de aquellas en las cuales está presente el antecedente \textbf{o} el consecuente. El soporte de un conjunto decrece en la medida que el número de elementos que contiene aumenta}.

La \textit{confianza} de una regla de asociación $X \Rightarrow Y$, denotada por $\mathit{conf}(X \Rightarrow Y)$, se define como $$\mathit{conf}(X \Rightarrow Y) = \frac{\mathit{supp}(X \cup Y)}{\mathit{supp}(X)}\text{,}$$ es decir, indica en qué fracción de las transacciones en las cuales está presente el antecedente la regla se cumple (i.e. está presente también el consecuente de la regla).

El \textit{lift} de una regla de asociación $X \Rightarrow Y$, denotado por $\mathit{lift}(X \Rightarrow Y)$, se define como $$\mathit{lift}(X \Rightarrow Y) = \frac{\mathit{conf}(X \Rightarrow Y)}{\mathit{supp}(Y)} = \frac{\mathit{supp}(X \cup Y)}{\mathit{supp}(X) \times \mathit{supp}(Y)}\text{.}$$ La intuición detrás del concepto de lift tiene lugar al interpretar las medidas descritas anteriormente desde un punto de vista probabilístico. Tomando el conjunto $\mathcal{T}$ como un universo de posibles resultados, o espacio muestral, se tiene que $$\mathit{supp}(X) = P(X) \quad \text{y} \quad \mathit{conf}(X \Rightarrow Y) = P(Y| X)\text{.}$$ Desde este punto de vista, la medida de lift indica qué tan bien la presencia del antecedente de una regla lograría predecir la presencia del consecuente. Por lo tanto, si la presencia del antecedente y del consecuente en una transacción cualquiera son eventos estadísticamente independientes (i.e. la ocurrencia de uno no afecta la probabilidad de que el otro ocurra), se tendrá que $\mathit{lift}(X \Rightarrow Y) = 1$; y este valor irá variando en la medida que ambos eventos sean más dependientes entre sí.

Por ejemplo, supongamos que se tiene el siguiente conjunto de transacciones

\begin{tabular}{l l}
\textbf{TID} & \textbf{Items} \\
1 & $a$, $c$ \\
2 & $a$, $d$ \\
3 & $b$, $c$ \\
4 & $b$, $d$ \\
\end{tabular}

donde \textit{TID} es el número identificador de la transacción. Luego, para este caso, se tiene que $$\mathit{lift}(\{a\} \Rightarrow \{c\}) = \frac{\mathit{supp}(\{a\} \cup \{c\})}{\mathit{supp}(\{a\}) \times \mathit{supp}(\{c\})} = \frac{1/4}{1/2 \times 1/2} = 1\text{,}$$ lo cual indica que la que la ocurrencia de que una transacción cualquiera satisfaga $\{a\}$ es estadísticamente independiente de que una transacción cualquiera satisfaga $\{b\}$.

En cambio, en el siguiente conjunto de transacciones

\begin{tabular}{l l}
\textbf{TID} & \textbf{Items} \\
1 & $a$, $c$ \\
2 & $a$, $d$ \\
3 & $b$, $c$ \\
4 & $b$, $c$ \\
\end{tabular}

se tiene que $$\mathit{lift}(\{a\} \Rightarrow \{c\}) = \frac{\mathit{supp}(\{a\} \cup \{c\})}{\mathit{supp}(\{a\}) \times \mathit{supp}(\{c\})} = \frac{1/4}{1/2 \times 3/4} = 2/3 < 1\text{,}$$ lo cual quiere decir que hay una mayor razón de transacciones que satisfacen $\{c\}$ dentro del total de transacciones que dentro del conjunto de transacciones que satisfacen $\{a\}$.

Finalmente, en el conjunto de transacciones

\begin{tabular}{l l}
\textbf{TID} & \textbf{Items} \\
1 & $a$, $c$ \\
2 & $a$, $d$ \\
3 & $b$, $d$ \\
4 & $b$, $d$ \\
\end{tabular}

se cumple que $$\mathit{lift}(\{a\} \Rightarrow \{c\}) = \frac{\mathit{supp}(\{a\} \cup \{c\})}{\mathit{supp}(\{a\}) \times \mathit{supp}(\{c\})} = \frac{1/4}{1/2 \times 1/4} = 2 > 1\text{,}$$ lo cual indica que hay una mayor razón de transacciones que satisfacen $\{c\}$ dentro del conjunto de transacciones que satisfacen $\{a\}$ que dentro del total de transacciones.

\subsection{Algoritmos principales}

\subsubsection{Algoritmo \textit{Apriori}}

En el mismo artículo seminal de ARL por Agrawal et al. \cite{agrawal1993mining}, se presentó el algoritmo \textit{Apriori}. El algoritmo \textit{Apriori} recibe como entrada un conjunto de transacciones, y tiene como objetivo encontrar y retornar todos aquellos conjuntos presentes que cumplan con el requisito de soporte mínimo indicado, también llamados \textit{conjuntos frecuentes}. Este algoritmo hace uso de las propiedades de clausura descendiente de la frecuencia de los conjuntos con respecto a sus subconjuntos con el fin de optimizar el proceso de generación de conjuntos de ítems frecuentes.

Por ejemplo, supongamos que se cuenta con un conjunto de transacciones, y que cada una contiene ítems pertenecientes a un universo de solo 4 posibles, $\mathcal{I} = \{0,\,1,\,2,\,3\}$. Luego, en principio, para extraer los conjuntos frecuentes a partir de estas transacciones, por cada uno de los conjuntos que es posible generar con este universo de 4 ítems posibles (llamados \textit{conjuntos candidatos}), se debe recorrer cada una de las transacciones, ver si la transaccion satisface este conjunto, y de ser así incrementar un contador. Luego de terminar este proceso para cada uno de los conjuntos posibles, se tendrá el número de veces que cada uno de estos se encuentra dentro del conjunto de transacciones, y teniendo el número total de estas, se puede obtener de forma directa el soporte de estos conjuntos. Por ejemplo, en la Figura \ref{fig:possible_itemsets} se observa todos los conjuntos candidatos que se pueden generar a partir de $\mathcal{I}$.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.7\textwidth]{imagenes/possible_itemsets.png}
\end{center}
\vspace*{-5mm}
\caption{Grafo que muestra todos los conjuntos posibles generados a partir del conjunto universo \{0, 1, 2, 3\}\cite{harrington2012machine}}
\label{fig:possible_itemsets}
\end{figure}

El problema radica en que el número de conjuntos candidatos crece de manera exponencial en el número de ítems del universo posible. En efecto, si el número de ítems del universo es $n$, entonces a partir de este es posible generar $2^n+1$ conjuntos. Por tanto, para un universo de 100 elementos, existen nada menos que $1.26\times10^{30}$ conjuntos candidatos; y debe, por tanto, recorrerse el total de transacciones este número de veces.

No obstante, es posible reducir el número de conjuntos candidatos utilizando la propiedad de \textit{clausura descendiente} de los conjuntos frecuentes, tambien llamado \textit{principio Apriori}. Esta propiedad asegura que si un conjunto dado es, en efecto, frecuente, entonces necesariamente todos sus subconjuntos también lo son. O, expresado de forma recíproca, si un conjunto dado resulta no ser frecuente, entonces necesariamente todos sus superconjuntos tampoco lo son. Esta última expresión es la que resulta más relevante para nuestro caso. Esto implica que luego de generar un conjunto candidato y verificar si es frecuente verificando el número de transacciones que lo satisfacen, si se comprueba que este conjunto no es frecuente (vale decir, no cumple con el requisito de soporte mínimo), entonces necesariamente ninguno de los superconjuntos posibles que lo contienen será frecuente, y por tanto no será necesario obtener sus soportes correspondientes contando el número de transacciones que los satisfacen\ref{fig:unfrequent_itemsets}.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.7\textwidth]{imagenes/unfrequent_itemsets.png}
\end{center}
\vspace*{-5mm}
\caption{Grafo que muestra todos los conjuntos posibles generados a partir del conjunto universo \{0, 1, 2, 3\}. Los conjuntos en gris son aquellos que de inmediato se sabe no son frecuentes si el conjunto \{2, 3\} resulta no serlo \cite{harrington2012machine}.}
\label{fig:unfrequent_itemsets}
\end{figure}

Esta propiedad permite reducir considerablemente el número de conjuntos candidatos y, por tanto, optimizar el algoritmo final; ya que no será necesario recorrer el total de transacciones tantas veces como se planteó originalmente. Para poder utilizar esta propiedad y beneficiarse de la optimización correspondiente, es necesario generar los conjuntos candidatos comenzando por aquellos que poseen menos elementos, y a partir de estos generar todos los superconjuntos posibles.

El algoritmo \textit{Apriori}, por lo tanto, en terminos generales resulta ser el siguiente

\begin{algorithm}[H]
\DontPrintSemicolon
\KwData{Conjunto de transacciones $\mathcal{T}$}
\KwResult{Conjunto de ítems frecuentes $\mathcal{L}$}
$\mathcal{L}_1 \leftarrow \{\text{conjuntos frecuentes de 1 solo ítem}\}$\;
\For{$k = 2; \, \mathcal{L}_{k-1} \neq \emptyset; \, {k++}$}{
	$C_k = \texttt{apriori-gen}(\mathcal{L}_{k-1})$\;
	$\forall \text{C} \in \mathcal{C}_k, \, \text{C.count} = 0$\;
	\For{transacciones $T \in \mathcal{T}$}{
	$\mathcal{C}_T = \texttt{subset}(\mathcal{C}_k,\,T)$\;
	\For{candidatos $C \in \mathcal{C}_T$}{
	$\text{C.count} \leftarrow \text{C.count} + 1$\;
	}
	}
	$\mathcal{L}_k = \{C \in \mathcal{C}_k \, : \, \text{C.count} \geq \text{minsup}\}$\;
}
$\mathcal{L} \leftarrow \bigcup_k \, \mathcal{L}_k$\;
\caption{Algoritmo \textit{Apriori}}
\end{algorithm}

Donde $\mathcal{L}_k$ corresponde a la colección de conjuntos frecuentes con $k$ elementos, los cuales tienen un contador asociado; y $\mathcal{C}_k$ consiste en la colección de conjuntos candidatos con $k$ elementos, que tienen también un contador asociado. La función \texttt{apriori-gen} es la encargada de generar una colección de conjuntos frecuentes de tamaño $k+1$ a partir de una colección de conjuntos candidatos datos de tamaño $k$. La función \texttt{subset} se encarga de recibir una colección de conjuntos de ítems frecuentes $\mathcal{C}_k$ y una transacción $T$ y de retornar una colección de ítems $\mathcal{C}_T = \{C \in \mathcal{C}_k \, : \, C \subseteq T\}$.

Posteriormente, se lleva a cabo la extracción de reglas a partir de la colección de conjuntos frecuentes $\mathcal{L}$. Para ello, se utiliza el algoritmo \textit{Apriori} de generación de reglas, que se detalla a continuación.

\begin{algorithm}[H]
\DontPrintSemicolon
\KwData{Colección de conjuntos frecuentes $\mathcal{T}$}
\KwResult{Colección de reglas de asociación $\mathcal{R}$}
\SetKw{Yield}{yield}
\SetKwFunction{GenRules}{genRules}
\ForAll{conjuntos frecuentes $l_k \in \mathcal{T}$, $k \geq 2$}{
	\Yield $\mathcal{R}$ = \GenRules{$l_k$, $l_k$}\;
}
\caption{Algoritmo \textit{Apriori} de generación de reglas}
\end{algorithm}

\begin{procedure}[H]
\DontPrintSemicolon
\SetKwFunction{Support}{support}
\SetKw{Yield}{yield}
\SetKwFunction{GenRules}{genRules}
\caption{genRules($l_k$: conjunto de $k$ ítems, $a_m$: conjunto de $m$ ítems)}
$A = \{a_{m-1} \, : \, a_{m-1} \subset a_m\}$\;
\ForAll{$a_{m-1} \in A$}{
	conf = \Support{$l_k$}/\Support{$a_{m-1}$}\;
	\If{conf $\leq$ minconf}{
		\Yield regla $a_{m-1} \Rightarrow (l_k - a_{m-1})$, con confianza = conf y soporte = \Support{$l_k$}\;
		\If{$m-1 > 1$}{
			\GenRules{$l_k$, $a_{m-1}$}\;
		}
	}
}
\end{procedure}

Básicamente, lo que hace el algoritmo \textit{Apriori} de generación de reglas es recibir una colección de conjuntos frecuentes de distintos tamaños, e invocar al procedimiento \texttt{genRules} sobre pares de conjuntos que sean del mismo tamaño $k$. A su vez, el procedimiento \texttt{genRules} recibe dos conjuntos cualquiera de ítems $l_k$ y $a_m$, obtiene todos los subconjuntos de $a_m$ de tamaño $m-1$ ($a_{m-1}$), y genera reglas que tengan a estos $a_{m-1}$ como antecedente y la diferencia entre $l_k$ y $a_{m-1}$ como consecuente; siempre y cuando cumplan con el requisito de confianza mínima. Luego, si $m$ sea mayor o igual a 2, el procedimiento se llama a sí mismo recursivamente con $l_k$ y $a_{m-1}$ como argumento, con el fin de generar esta vez reglas cuyo antecedente tenga $m-1$ elementos. De esta manera se generan reglas con todas las combinaciones posibles de antecedente y consecuente que tengan la confianza mínima indicada.

La teoría indica que la complejidad del algoritmo Apriori está acotada por $\mathcal{O}(\mathcal{C}_{\mathit{sum}} \times |\mathcal{T}|)$, donde $\mathcal{C}_{\mathit{sum}}$ es la suma de los tamaños del total de conjuntos candidatos considerados y $|\mathcal{T}|$ denota el tamaño del conjunto de transacciones.

\subsubsection{Algoritmo \textit{FP-Growth}}

Más recientemente, Han et al. introdujeron el uso de una estructura de datos llamada \textit{Frequent Pattern Tree} \cite{han2004mining} en la extracción de conjuntos de ítems frecuentes a partir de conjuntos de transacciones. Con esto dieron origen al algoritmo \textit{FP-Growth}.

Un \textit{Frequent Pattern Tree (FP-Tree)} es una estructura de datos de tipo árbol, que consiste en un nodo raíz que tiene como sus hijos a \textit{sub-árboles de prefijos de ítems}. Cada nodo del \textit{sub-árbol de prefijo de ítem} contiene tres campos: el nombre del ítem al cual el nodo representa, un contador que registra el número de transacciones que satisfacen la rama del árbol que va de la raíz hasta este nodo, y un puntero al siguiente nodo del FP-Tree que contenga el mismo nombre de ítem o un puntero vacío si no existe tal nodo.

A su vez, el FP-Tree posee una estructura de datos auxiliar denominada \textit{tabla de encabezados}. Cada entrada en esta tabla posee dos campos. El primero es el nombre del ítem y el segundo es un puntero al primer nodo del FP-Tree que posee el mismo nombre de ítem.

Supongamos, por ejemplo, que se cuenta con el siguiente conjunto de transacciones:

\begin{tabular}{l l}
\textbf{TID} & \textbf{Ítemes} \\
1 & $r$, $z$, $h$, $j$, $p$ \\
2 & $z$, $y$, $x$, $w$, $v$, $u$, $t$, $s$ \\
3 & $z$ \\
4 & $r$, $x$, $n$, $o$, $s$ \\
5 & $y$, $r$, $x$, $z$, $q$, $t$, $p$ \\
6 & $y$, $z$, $x$, $e$, $q$, $s$, $t$, $m$ \\
\end{tabular}

Supongamos que se desea extraer de estas transacciones aquellos conjuntos frecuentes que cumplan un soporte mínimo de 0.5; vale decir, en este caso, que estén presentes en al menos 3 transacciones. El procedimiento para generar el FP-Tree es, entonces, el siguiente. En primer lugar, se extrae a partir de las transacciones todos los ítems presentes y se ordenan por orden de frecuencia. En este caso, el resultado es el conjunto $I = \{z, \, r, \, x, \, y, \, s, \, t, \, p, \, q, \, h, \, j, \, w, \, v, \, u, \, n, \, o, \, e, \, m\}$. Luego, se elimina de este conjunto todos aquellos ítems que no cumplan con el requisito mínimo de soporte deseado, obteniendo como resultado $I = \{z, \, r, \, x, \, y, \, s, \, t\}$

Luego, en cada transacción se ordenan sus conjuntos de ítems según su frecuencia, y posteriormente se filtran aquellos ítems que no cumplan con el soporte mínimo deseado. Se genera, de esta forma, un nuevo conjunto de transacciones cuyos ítems se encuentran ordenados según frecuencia y que poseen solamente ítems frecuentes, como se observa en la siguiente tabla:

\begin{tabular}{l l l}
\textbf{TID} & \textbf{Ítemes originales} & \textbf{Ítemes ordenados y filtrados} \\
1 & $r$, $z$, $h$, $j$, $p$ & $z$, $r$ \\
2 & $z$, $y$, $x$, $w$, $v$, $u$, $t$, $s$ & $z$, $x$, $y$, $s$, $t$ \\
3 & $z$ & $z$ \\
4 & $r$, $x$, $n$, $o$, $s$ & $x$, $s$, $r$ \\
5 & $y$, $r$, $x$, $z$, $q$, $t$, $p$ & $z$, $x$, $y$, $r$, $t$ \\
6 & $y$, $z$, $x$, $e$, $q$, $s$, $t$, $m$ & $z$, $x$, $y$, $s$, $t$ \\
\end{tabular}

Una vez listo esto, se puede comenzar con el procedimiento de construcción del árbol en sí. Se comienza por insertar el nodo raíz, cuyo nombre es vacío o \textit{null}. Luego se comienza a añadir los conjuntos frecuentes a partir de las transacciones con ítems ordenados y filtrados. Estas son sucesivamente añadidas al árbol de tal manera que cada ítem resulte ser hijo del ítem anterior según el orden en que se encuentra en la transacción. Ahora bien, si el ítem a añadir ya se encuentra presente como hijo del nodo actual, entonces en vez de agregar un nuevo nodo con el mismo nombre, simplemente se incrementa el contador del nodo ya existente y se inserta el siguiente ítem en la transacción como hijo de este. En la Figura \ref{fig:fptree_process} se aprecia parte de este proceso.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.7\textwidth]{imagenes/fptree_process.png}
\end{center}
\vspace*{-5mm}
\caption{Proceso de construcción del FP-Tree del ejemplo. Aquí se puede apreciar cómo ocurre el mecanismo de bifurcación de ramas del FP-Tree al insertar las dos primeras transacciones \cite{harrington2012machine}.}
\label{fig:fptree_process}
\end{figure}

En la Figura \ref{fig:fptree_header} se muestra el FP-Tree y la tabla de encabezados que se obtiene al final de llevar a cabo el proceso de construcción con los datos de ejemplo.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.7\textwidth]{imagenes/fptree_header.png}
\end{center}
\vspace*{-5mm}
\caption{FP-Tree y headertables construidos a partir de los datos de ejemplo, con un soporte de 0.5 \cite{harrington2012machine}.}
\label{fig:fptree_header}
\end{figure}

Luego, se procede a extraer los conjuntos frecuentes a partir del FP-Tree, para lo cual no es necesario hacer uso del conjunto de transacciones originales. Este proceso de extracción consta de los siguientes pasos:

\textbf{1. Obtener conjunto de patrones condicionales a partir del FP-Tree.}

Por cada uno de los conjuntos frecuentes de un solo elemento presentes en la tabla de encabezados, se extrae su \textit{conjunto de patrones condicionales}, que es una colección de \textit{ramas de prefijo}. Cada una de estas ramas de prefijo es un conjunto de ítems presentes en un camino del árbol original que termina en un cierto ítem. Vale decir, son todos los ítems que se encuentran desde la raíz del árbol hasta justo antes del nodo correspondiente a un ítem dado. A cada uno de estos va asociado, además, un contador que posee el mismo valor del mismo contador presente en el nodo donde está presente aquel ítem. Entonces, por ejemplo, el conjunto frecuente \{$r$\} se encuentra tres veces en el árbol original. Por lo tanto hay tres \textit{ramas de prefijo}; vale decir, tres caminos que van desde el nodo raíz hasta un nodo que contenga a \{$r$\}. Por lo tanto, se tiene que su conjunto de patrones condicionales es una colección que contiene tres conjuntos o ramas de prefijo: \{$x$, $s$\}:1, \{$z$, $x$, $y$\}:1 y \{$z$\}:1 (el número después de los dos puntos indica el contador asociado a $r$ en su nodo correspondiente); tal y como se muestra en la Figura \ref{fig:cond_pat}.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.35\textwidth]{imagenes/cond_pat.png}
\end{center}
\vspace*{-5mm}
\caption{Conjunto de ítems que forman parte del conjunto de patrones condicionales del conjunto \{$r$\}; vale decir, todos aquellos que están presentes desde la raíz del árbol hasta justo antes de donde esté presente el ítem $r$. Por tanto, en este caso el conjunto de patrones condicionales de \{$r$\} lo conforman los conjuntos \{$x$, $s$\}, \{$z$, $x$, $y$\} y \{$z$\}.}
\label{fig:cond_pat}
\end{figure}

\textbf{2. A partir de una de sus ramas de prefijo, construir un FP-Tree condicional.}

A partir de las ramas de prefijo que se encuentran dentro del conjunto de patrones condicionales se construye, entonces, un nuevo FP-Tree de la misma manera que se construyó el original a partir de las transacciones. A este nuevo FP-Tree se le denomina \textit{FP-Tree condicional}. Tal como en el caso original, este FP-Tree contiene solo los ítems que cumplen con el requisito de soporte mínimo dentro de la rama de prefijo actual. Luego, se añade a la colección de conjuntos frecuentes los conjuntos que consisten de la unión entre los ítems que se encuentran en la tabla de encabezados y el prefijo a partir del cual se obtuvieron las ramas de prefijo con las que se construyó el FP-Tree actual.

Para ilustrarlo mejor, continuemos con el mismo ejemplo de la parte anterior. Como se mostró, las ramas de prefijo asociadas al conjunto \{$r$\} conforman el conjunto de patrones condicionales \{$x$, $s$\}:1, \{$z$, $x$, $y$\}:1 y \{$z$\}:1. Sabemos desde ya que \{$r$\} es un conjunto frecuente, y, por lo tanto, lo agregamos a nuestra colección de conjuntos frecuentes. A partir de estos crearemos un nuevo árbol similar al original; un \textit{FP-Tree condicional}. Trataremos este conjunto de patrones condicionales como si fuese un conjunto de transacciones, y el procedimiento es similar al original, salvo que en esta ocasión los ordenaremos en forma inversa; vale decir, del ítem menos frecuente al más frecuente. El resultado es el siguiente:

\begin{tabular}{l l l}
\textbf{Contador} & \textbf{Patrón condicional ordenado} \\
1 & $s$, $x$ \\
1 & $y$, $x$, $z$ \\
1 & $z$ \\
\end{tabular}

Posteriormente, y al igual que en el procedimiento original, procedemos a calcular el soporte de cada ítem y eliminar aquellos que no cumplan con el soporte de 0.5; o sea, que no estén presentes en al menos 3 transacciones. La diferencia es que, esta vez, tenemos cuidado de considerar además el contador asociado a cada patrón condicional. Como se aprecia en la tabla anterior, \{$r$\} e \{$y$\} están presentes en 1 patrón condicional (cada uno con un contador de 1 asociado), \{$x$\} y \{$z$\} sólo en 2. Por lo tanto, el árbol generado por este conjunto de patrones es vacío.

Intentemos ahora con otro conjunto frecuente de un ítem: \{$t$\}. Sabemos que de antemano que este es un conjunto frecuente, y, por ende, lo agregamos a la colección de conjuntos frecuentes. Luego, procedemos a obtener el conjunto de patrones condicionales de \{$t$\}. Este corresponde a \{$z$, $x$, $y$, $s$\}:2 y \{$z$, $x$, $y$, $r$\}:1. Al igual que en la iteración anterior, ordenamos en sentido creciente de soporte y filtramos los ítemes no frecuentes del conjunto de patrones condicionales. El resultado de esto es:

\begin{tabular}{l l l}
\textbf{Contador} & \textbf{Patrón cond. ordenado} & \textbf{Patrón cond. ordenado y filtrado} \\
2 & $s$, $y$, $x$, $z$ & $y$, $x$, $z$ \\
1 & $r$, $y$, $x$, $z$ & $y$, $x$, $z$ \\
\end{tabular}

Se puede observar en la tabla que se eliminaron los subconjuntos \{$s$\} y \{$r$\} por tener solamente un contador en total de 2 y 1, respectivamente, en sus patrones condicionales. Luego, con los patrones resultantes se procede a construir un FP-Tree condicional, como se observa en la Figura \ref{fig:cond_fptree}.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.7\textwidth]{imagenes/cond_fptree.png}
\end{center}
\vspace*{-5mm}
\caption{Proceso de construcción de un FP-Tree condicional a partir del conjunto de patrones condicionales \{$y$, $x$, $z$\}:2 y \{$y$, $x$, $z$\} \cite{harrington2012machine}.}
\label{fig:cond_fptree}
\end{figure}

\textbf{3. Repetir los pasos anteriores de manera recursiva hasta que el FP-Tree condicional actual tenga un solo elemento.}

Una vez hecho esto, se repite el proceso de manera recursiva, construyendo un nuevo FP-Tree condicional a partir de las ramas de prefijo para cada uno de los ítems frecuentes presentes en los patrones condicionales hasta que el FP-Tree construido en la recursión actual sea vacío.

Siguiendo con el ejemplo, el FP-Tree condicional resultante, que se observa en el extremo derecho de la Figura \ref{fig:cond_fptree}, esta compuesto de los conjuntos de ítemes frecuentes \{$y$\}, \{$x$\} y \{$z$\}; cada uno de ellos con un contador total asociado de 3. Para el paso recursivo, lo que se hace es seleccionar uno de estos conjuntos y realizar una unión con el conjunto original, \{$t$\}. De esta forma, si seleccionamos \{$z$\}, el conjunto resultante será \{$z$, $t$\}. Luego, procedemos a agregar este conjunto a nuestra colección de conjuntos frecuentes. Y una vez hecho esto, procedemos a extraer el conjunto de patrones condicionales de \{$z$\} en el FP-Tree condicional. Al hacerlo se obtiene sólamente el conjunto \{$y$, $x$\}:2. A partir de este conjunto se vuelve a construir un FP-Tree condicional como en el paso anterior. Esta recursión se repite hasta que el FP-Tree generado sea vacío.

Una vez finalizadas las recursiones sobre todos los conjuntos de ítemes de un elemento, se procede a retornar la colección de conjuntos frecuentes resultante; en la cual fuimos guardando cada uno de los conjuntos frecuentes que encontramos en el proceso recursivo.

La complejidad teórica del algoritmo \textit{FP-Growth} resulta ser más difícil de expresar analíticamente que la del algoritmo \textit{Apriori}, ya que depende tanto del número de ítemes frecuentes, su frecuencia y de la profundidad del FP-Tree generado en cada paso, entre otros. Esto último, a su vez, depende de las características internas de los datos más bien que solamente de la cantidad de estos. Junto con esto, el algoritmo realiza operaciones de diverso tipo, como comparaciones, recorrer nodos, inicializar estructuras de datos, y no solamente recorrer los datos originales \cite{kosters2003complexity}.

\subsection{Otros algoritmos, implementaciones y aplicaciones}

Posteriormente, Agrawal et al. presentaron el algoritmo \textit{AprioriTid}, cuyas mejores características fueron combinadas con el algoritmo \textit{Apriori} para crear el algoritmo \textit{AprioriHybrid}, de orden de complejidad lineal en el número de transacciones \cite{agrawal1994fast}. Luego se han realizado más desarrollos en ARL orientado a transacciones secuenciales de clientes de puntos de ventas \cite{agrawal1995mining}.

Savasere et al. introdujeron el algoritmo \textit{Partition} \cite{savasere1995efficient} con el fin de extraer reglas de asociación en base de datos, el cual presenta reducciones en las operaciones de la CPU y de entrada/salida, y que además facilita la paralelización. Posteriormente se creó el algoritmo \textit{Dynamic Itemset Counting (DIC)} \cite{brin1997dynamic}, que realiza menos lecturas sobre los datos que los algoritmos previos, y que utiliza la métrica de \textit{Convicción} a la hora de generar reglas de asociación. Luego, Park et al. presentaron un algoritmo que hace uso de funciones de Hashing con el fin de generar reglas candidatas \cite{park1995effective}. Se han realizado, también, adaptaciones de los algoritmos previos con el fin de realizar ARL en datos de tipo cuantitativo \cite{srikant1996mining}.

Esfuerzos posteriores se han realizado con el fin de profundizar en los fundamentos teóricos subyacentes en ARL (e.g. definiendo el conjunto de posibles ítems como una estructura algebráica llamada \textit{retículo}) \cite{zaki1998theoretical}, y con el fin de extender la noción de reglas de asociación a correlaciones \cite{brin1997beyond} y taxonomías \cite{srikant1996mining}.

Luego de esto, se han hecho numerosas implementaciones y optimizaciones a los algoritmos más utilizados en ARL, como, por ejemplo, el algoritmo Apriori \cite{bodon2010fast}; así como implementaciones que facilitan el mantener la privacidad de cada una de las fuentes de datos que participan en el proceso \cite{evfimievski2004privacy}.

Desde su concepción, el método de ARL ha sido aplicado en numerosas áreas, tales como la detección de intrusiones \cite{lee2000framework} y anomalías \cite{patcha2007overview} \cite{chandola2009anomaly}, educación \cite{romero2007educational} \cite{romero2008data}, química \cite{dehaspe1998finding}, privacidad de datos \cite{ghinita2008private}, búsqueda en la web \cite{ferragina2008personalized}, tráfico en redes \cite{estan2003automatically}, computación social \cite{li2008tag}, búsqueda semántica \cite{cohen2007associative}, biología \cite{kramer2001molecular} \cite{carmona2007genecodis}, salud \cite{karabatak2009expert} \cite{chaves2011efficient}, medios de comunicación \cite{davidson2010youtube} \cite{kobilarov2009media}, y la investigación forense \cite{iqbal2013unified}. Junto con esto, se han realizado numerosas investigaciones sobre el estado actual de ARL y sus posibles desarrollos a futuro dentro del marco de métodos automatizados de generación de conocimiento \cite{han2007frequent}.

Si bien existen numerosos esfuerzos por utilizar minería de datos y Machine Learning en diversos ámbitos de la astronomía (en particular, en detección, clasificación y caracterización de líneas moleculares en espectros de emisión \cite{vskoda2011searching}), hasta la fecha no se ha propuesto abiertamente el uso de ARL sobre datos extraídos de espectros de frecuencia.

Sin embargo, se han realizado avances en ampliar los conceptos subyacentes en ARL con el fin de aplicar el método en campos más diversos \cite{brin1997beyond}. Específicamente, una rama de investigación ha desarrollado lo que se denomina \textit{Weighted Association Rule Learning} \cite{wang2000efficient} \cite{cai1998mining}. Este método permite asociar medidas de interés arbitrario a priori a ciertos conjuntos de datos. Si bien esto hace que se pierdan propiedades de clausura que son útiles a la hora de generar algoritmos eficientes, permite trabajar con conjuntos de transacciones de los cuales a algunos se desea dar más relevancia, a la hora de generar reglas de asociación, que a otros.

